## Bagging

Bagging refers to the process of training classifiers on different datasets, to create a collection of different classifiers. The classifiers are different because each dataset is different. A classifier with less **stability** will be sensitive to the characteristics of a particular dataset, and will have a slightly different idea about the mapping from **features** to **labels**. When the combined classifier is applied, the different ideas will combine by voting to create a higher quality final prediction. 

We want the datasets we train the classifiers on to be similar to the training set, but not exactly the same. We would like the datasets to capture the same underlying pattern, but not have exactly the same details. Each classifier will then be a little different, especially if the classifier is not stable: it will pick up the idiosyncrasies of the particular dataset. This method gives us a collection of different classifiers. 

We generate the different datasets (called **bags**) used in bagging by resampling. We start with an empty bag. We then loop until we have n examples, where n is usually the size of the original dataset. At every step in the loop, we have an equal probability of choosing any datapoint. At the end of the loop, we have a collection of n data points. Note that some points will appear more than once, since at each set we have an equal probability of choosing any point, it is possible we will have added it more than once. For this reason, this procedure is called **resampling with replacement**. Also, some points will never appear in the bag, about 1/3rd of the points. These can be used to evaluate the accuracy of the classifier, similarly to how we use left out data in **cross validation**. 

**Resampling with replacement** to improve a method is a general technique in statistics known as the bootstrap. **Bootstrap** works because combining multiple estimates reduces bias and variance - when applied to classifiers in bagging, it reduces the bias and compensates for the fact that the classifier may be unstable. The name **bagging** is a contraction of **bootstrap aggregating**: bootstrap for generating the bags, and aggregation (voting) to describe how the different classifiers combine to make predictions. 
