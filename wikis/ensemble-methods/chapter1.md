## What are Ensemble Methods?

Ensemble methods are a general technique that can be used to improve the performance of any algorithm. Ensemble methods use a number of different models to generate predictions. Each model generates different predictions, and the collection of models all working together on common data, will overcome the failings of any particular model. The underlying principle behind any ensemble method is that a number of different models are all used on the same data. Some kind of voting mechanism is then applied to decide the final output of the overall method.

Every individual machine learning method has some **bias**: given a particular dataset, different methods will prefer certain **decision boundaries**. The bias cannot be eliminated, as without some kind of bias, learning is not possible. However, too much bias may also hurt us, and it is important to control the amount of bias in our method. 

Each classifier has its own particular model. Recall that the model is the explicit mathematical relationship between **feature space** and **label space**. As a result, a model is a specific idea about how and what to look for in the data to classify correctly. Ensemble methods take their strength from the fact that they try a large set of competing ideas, and decide for any specific dataset, which ideas work best.

Another problem that can affect an individual classifier is **stability**. This refers to the fact that a method will perform very differently for different datasets. Some classifiers will, when trained, produce remarkably different parameters for slight variations on the same dataset. An associated problem is **overfitting**. This is when a method has become too closely tuned to the inherent noise in one particular dataset, as opposed to the true underlying patterns that dataset has in common with all data of that type.

The amounts by which these problems affect a method depend on the chosen model, which is to say that stability and bias occur in different ratios for different ideas as to what the relationship between features and labels should be. To prevent these problems, we can train a few different machine learning models, or equivalently, try a few different ideas, and then vote based on which ideas work best for our data. This is where the idea of ensemble methods came from. 

Ensemble methods are an average of several models. An ensemble of identical models would produce the same predictions (by voting) as the model by itself, giving no improvement in using an ensemble vs using a single model. The greatest improvement will come when the models have different ideas the relationship between features and labels. Any group of models which vote to make predictions is an ensemble method. One example occurs sometimes in machine learning contests, when the average of the top performing models can out-perform any individual model. This requires several good models and perhaps tuning the amount to which each individual model contributes to the prediction: one model may be allocated more “votes” than the others if we have reason to believe that it is a better model. 

In this wiki, we build on this simple idea, with a few methods to generate classifiers that will work well when combined together. One particular method for improving an ensemble method is called **bagging** - **bootstrap aggregation**. These methods perform especially well when applied to decision trees, which is the basis for the technique known as **random forests**.
