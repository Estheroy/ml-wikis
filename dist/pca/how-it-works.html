
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>How it Works? Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="application-of-pca.html" />
    
    
    <link rel="prev" href="theory-and-knowledge.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="what-is-pca.html">
            
                <a href="what-is-pca.html">
            
                    
                    What is PCA?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="theory-and-knowledge.html">
            
                <a href="theory-and-knowledge.html">
            
                    
                    Theory and Knowledge
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4" data-path="how-it-works.html">
            
                <a href="how-it-works.html">
            
                    
                    How it Works?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="application-of-pca.html">
            
                <a href="application-of-pca.html">
            
                    
                    Application of PCA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="limitation-and-extension-of-pca.html">
            
                <a href="limitation-and-extension-of-pca.html">
            
                    
                    Limitation and Extension of PCA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="eigenvectors-and-eigenvalues.html">
            
                <a href="eigenvectors-and-eigenvalues.html">
            
                    
                    Eigenvectors and Eigenvalues
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="reference.html">
            
                <a href="reference.html">
            
                    
                    Reference
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >How it Works?</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="how-it-works">How it Works?</h2>
<p>PCA finds the direction of greatest information by computing the directions of greatest <em>variance</em>, and then <em>projecting</em> the dataset onto vectors that correspond to those directions. This is because redundancy can be quantified with variance and covariance.</p>
<p>The directions of greatest variance in a dataset are given by the <em>eigenvectors</em> of the <em>covariance matrix</em> of that dataset, which in turn is given by <em>variance and covariance</em>. <strong>Variance</strong> is a measure of the spread of one variable. <strong>Covariance</strong> is a measure of the spread between two variables. Formally, the two are similar:</p>
<p><script type="math/tex; ">var(x) = \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}{n}</script></p>
<p><script type="math/tex; ">cov(x,y) = \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{n}</script></p>
<p>Here, <script type="math/tex; ">\bar{x}</script> and <script type="math/tex; ">\bar{y}</script> refer to the mean of <script type="math/tex; ">x</script> and <script type="math/tex; ">y</script>, respectively.</p>
<p>The <strong>covariance matrix</strong>, which is central to PCA, is composed of all variance and covariance measures of our dataset. The <script type="math/tex; ">(i,j)^{th}</script> element of the matrix is the covariance between i-th and j-th variable. For 3 dimensions, the covariance matrix C is given by:</p>
<p><script type="math/tex; ">C = \begin{bmatrix}
Cov(X,X) & Cov(X,Y) & Cov(X,Z)\\ 
Cov(Y,X) &  Cov(Y,Y)&  Cov(Y,Z)\\ 
Cov(Z,X) &  Cov(Y,Z)& Cov(Z,Z)
\end{bmatrix}</script></p>
<p>Where <script type="math/tex; ">Cov(X,X)</script> refers to <script type="math/tex; ">Var(X)</script>. Notice that for a D-dimensional dataset, we will always have a DxD covariance matrix.</p>
<p>A matrix A has an <strong>eigenvector </strong>v with the <strong>associated eigenvalue</strong> <script type="math/tex; ">\lambda</script> if<script type="math/tex; ">Av = \lambda v</script></p>
<p>where A is a square matrix, v is a column vector, and <script type="math/tex; ">\lambda</script> is a scalar. For an NxN square matrix, there are always N eigenvectors with associated eigenvalues for that matrix.</p>
<p>Eigenvectors have many useful properties that are leveraged in science and engineering, but the exact reasons behind why they&#x2019;re so essential to PCA is outside the scope of this article. It will suffice to say that the eigenvectors of the covariance matrix correspond to the directions of ranked variance described above. Specifically, the eigenvector of the covariance matrix with the largest eigenvalue always points to the direction of greatest variance in the data. This is called the first eigenvector. The second eigenvector points in the direction that has the largest variance of all those directions that are orthogonal to the first eigenvector. The third eigenvector, correspondingly, points in the direction that has the largest variance of all directions orthogonal to both the first and second eigenvectors, and so on. Once found, the eigenvectors are sorted by eigenvalue, and the resulting vectors are known as the <strong>principal components</strong> of a dataset.</p>
<p>Projecting a dataset onto a new axis can be accomplished with matrix multiplication. Specifically, for a matrix D, with rows as dimensions and columns as data points, along with a basis V, whose columns are directions we are mapping onto, the re-orientation, or projection, T, of the dataset D by the columns of V is given by the product:</p>
<p><script type="math/tex; ">V^{*} D = T</script></p>
<p>Where the asterisk mark denotes a transpose.</p>
<p>Because the eigenvectors are orthogonal, they form an ideal basis, and because they are ranked by variance, they inform the user as to which directions can be dropped once the data has been redefined in component space. This means that the resulting matrix, T, which has the same shape as D, can be reduced by the N dimensions of least variance, simply by removing its bottom N rows.</p>
<p>If this is performed, data can be left in the projected space, called the component space, or can be projected back into its original dimensionality, a process can <strong>reconstructing</strong>. If a reduced dataset is reconstructed back into its original basis, it will be, in its original space, flattened along the directions of least variance. This latter process is often referred to as <strong>de-noising</strong>.</p>
<p><img src="assets/pca7.png" alt=""></p>
<p>We can distill PCA into the following steps:</p>
<p>Let our original data be the following plot</p>
<h4 id="1-&#x2013;-subtract-the-mean-vector-from-each-of-the-dimensions">1 &#x2013; Subtract the mean vector from each of the dimensions</h4>
<p>This produces a dataset whose mean is zero called, called Zero Mean Data. Subtracting the mean simplifies the process of calculating the covariance matrix. This yields the plot on the right (note the axis label):</p>
<h4 id="2-&#x2013;-compute-principal-components-by-calculating-and-ranking-by-eigenvalue-the-eigenvectors-of-the-covariance-matrix">2 &#x2013; Compute principal components by calculating and ranking by eigenvalue the eigenvectors of the covariance matrix</h4>
<p>As described above, the eigenvectors and eigenvalues of the covariance matrix correspond to orthogonal directions of variance. When ordered by eigenvalue, they form the principal components. The following plot shows the same dataset, with the principal components directions highlighted in red and green.</p>
<h4 id="5-&#x2013;-project-the-data-into-component-space">5 &#x2013; Project the data into component space.</h4>
<p>This makes it so that the dimensions of our dataset are now ranked by variance. Specifically, the x-axis now aligns with widest spread, the y-axis with narrowest.</p>
<h4 id="6-&#x2013;-choose-and-apply-application">6 &#x2013; Choose and apply application.</h4>
<p>At this point, the user can choose to reduce dimensions or denoise.</p>
<p>Reducing dimensions is just a matter of dropping values of the least informative directions in in component space. This means a smaller dataset that preserves the data&#x2019;s original structure, seen in the following plot.</p>
<p>Reconstructing, on the other hand, means re-projecting the reduced data back to the original space. This doesn&#x2019;t make the set smaller to store, but it does remove noise and can improve successive steps in the project. The following plot illustrates this, and shows the original dataset in blue with its reconstruction in black.</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="theory-and-knowledge.html" class="navigation navigation-prev " aria-label="Previous page: Theory and Knowledge">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="application-of-pca.html" class="navigation navigation-next " aria-label="Next page: Application of PCA">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"How it Works?","level":"1.4","depth":1,"next":{"title":"Application of PCA","level":"1.5","depth":1,"path":"application-of-pca.md","ref":"application-of-pca.md","articles":[]},"previous":{"title":"Theory and Knowledge","level":"1.3","depth":1,"path":"theory-and-knowledge.md","ref":"theory-and-knowledge.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["autocover","mathjax"],"pluginsConfig":{"autocover":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"how-it-works.md","mtime":"2017-03-13T06:21:39.452Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-03-13T06:24:53.132Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

