
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>The Math behind Linear Regression Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="cost-function-and-equation-derivation.html" />
    
    
    <link rel="prev" href="supervised-learning.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Abstract
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="what-is-linear-regression.html">
            
                <a href="what-is-linear-regression.html">
            
                    
                    What is Linear Regression?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="supervised-learning.html">
            
                <a href="supervised-learning.html">
            
                    
                    Supervised Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4" data-path="the-math-behind-linear-regression.html">
            
                <a href="the-math-behind-linear-regression.html">
            
                    
                    The Math behind Linear Regression
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="cost-function-and-equation-derivation.html">
            
                <a href="cost-function-and-equation-derivation.html">
            
                    
                    Cost Function and Equation Derivation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="advantages-and-disadvantages.html">
            
                <a href="advantages-and-disadvantages.html">
            
                    
                    Advantages and Disadvantages
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >The Math behind Linear Regression</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="the-math-behind-linear-regression">The Math behind Linear Regression</h2>
<p>To perform supervised learning, we must decide how we&#x2019;re going to represent the function h. Specifically, we have to decide the exact class of functions that we believe are viable for the problem. We can choose a linear relationship, quadratic, logarithmic, etc, so long as the type of function we choose, which we will call our model, is appropriate for the data we&#x2019;re working with.</p>
<p>What we choose is the generic class of functions, for example, all linear functions, or all quadratic functions. We let the algorithm search through the space of all possible linear functions, or quadratic function, depending on our choice, and settle on that one particular function that performs best on our training set. This choice of ideal function from a class is often called <strong>fitting</strong> or <strong>training</strong> a model.</p>
<p align="center">
    <img src="assets/the-math-behind-linear-regression1.png">
</p>

<p>In the case of linear regression, we choose the class of functions known as linear functions. It would be the same, in common parlance, to say that we chose the linear model. Our goal with the linear model is to predict the labels based on the input variables, or elements in our x vectors. Using the following equation, we assert that the label of any point is a linear combination of that point:</p>
<p><script type="math/tex; ">y = h(x,w) = w_{0} + w_{1}x_{1} + w_{2}x_{2} + ... + w_{d}x_{d}</script></p>
<p>where x is a single input vector, x1,x2,xd are the elements of that vector, and w is a weight vector. The w&apos;s are the <strong>parameters</strong> of the linear model. Our goal requires that we find a set of Ws that fit our model assertion.</p>
<p>When there is no risk of confusion, we will drop the w in h(w, x), and write it as h(x). If we concatenate a 1 to the beginning of the x vector, and refer to this one as x0, then our linear function can be expressed as a dot product:</p>
<p><script type="math/tex; ">y = h(x) = \sum_{i = 0}^{n}w_{i}x_{i} = \vec{w^{T}}\cdot \vec{x}</script></p>
<p>Now, if our data happens to all fall on a line, then there exists some weights with which our model will fit perfectly. If, as is always the case, the data does not fall perfectly on a line, then there is no perfectly fitting vector of weights.</p>
<p>Our training algorithm, then, should find the &#x201C;best&#x201D; set of weights. We define the best set as those that minimize an error function, given by:</p>
<p><script type="math/tex; ">J(w) = \frac{1}{2}\sum_{i=1}^{m}(h(x_{i})-y^{i})^{2}</script></p>
<p>Where m is the number of datapoints in our dataset. This function takes, for some w, the difference between the label our model returns with that w, and the true label. This function is smallest when h(x) = y, or when our model is perfectly returning the label for some point. It can be seen as a sum of error for each point, and is called the <strong>sum-squared error</strong> function.</p>
<p align="center">
    <img src="assets/the-math-behind-linear-regression2.png">
</p>


                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="supervised-learning.html" class="navigation navigation-prev " aria-label="Previous page: Supervised Learning">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="cost-function-and-equation-derivation.html" class="navigation navigation-next " aria-label="Next page: Cost Function and Equation Derivation">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"The Math behind Linear Regression","level":"1.4","depth":1,"next":{"title":"Cost Function and Equation Derivation","level":"1.5","depth":1,"path":"cost-function-and-equation-derivation.md","ref":"cost-function-and-equation-derivation.md","articles":[]},"previous":{"title":"Supervised Learning","level":"1.3","depth":1,"path":"supervised-learning.md","ref":"supervised-learning.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["autocover","mathjax"],"pluginsConfig":{"autocover":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"the-math-behind-linear-regression.md","mtime":"2017-03-13T06:21:39.420Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-03-13T06:23:12.472Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

