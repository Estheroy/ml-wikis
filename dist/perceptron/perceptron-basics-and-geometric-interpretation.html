
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Perceptron basics and geometric interpretation Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="perceptron-learning-algorithm.html" />
    
    
    <link rel="prev" href="./" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="chapter1.html">
            
                <a href="chapter1.html">
            
                    
                    Abstract
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="./">
            
                <a href="./">
            
                    
                    What is the Perceptron?
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3" data-path="perceptron-basics-and-geometric-interpretation.html">
            
                <a href="perceptron-basics-and-geometric-interpretation.html">
            
                    
                    Perceptron basics and geometric interpretation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="perceptron-learning-algorithm.html">
            
                <a href="perceptron-learning-algorithm.html">
            
                    
                    Perceptron Learning Algorithm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="convergence-and-limitation.html">
            
                <a href="convergence-and-limitation.html">
            
                    
                    Convergence and Limitation
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >Perceptron basics and geometric interpretation</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="perceptron-basics-and-geometric-interpretation">Perceptron basics and geometric interpretation</h1>
<p>The perceptron is a binary classifier. For any input $$x\subset\Re^n$$, a perceptron maps that vector to the set $${0,1}$$. The perceptron is used in this way to map each point in some feature space to two possible labels. It is used for supervised learning tasks, where a set of or training points is provided together with a set of true labels. The perceptron can learn from these examples and assign labels to previously unseen points.</p>
<p>The perceptron is a parametric model in that, once it is trained, it has a set number of weights that it uses to assign labels to new points. This article covers the exact training process, by which an ideal set of weights is found, in a later section. For this section we will explain how a perceptron uses these weights, and what they correspond to mathematically and functionally. Note that the terms &#x201C;set of weights&#x201D; and &#x201C;weight vector&#x201D; are used interchangeably. </p>
<p>Part of the perceptron model is that it computes an explicit <strong>decision boundary</strong>, which is very closely related to the weights. A decision boundary is a line, plane, or hyper-plane, (depending on the dimensionality of the input,) that is used to partition the feature space into two disjoint regions corresponding to either class. Every point above a decision boundary is assigned one class, every point below it is assigned the other.</p>
<p><img src="assets/image25.png" width="300" height="300"></p>
<p>Training a perceptron is functionally equivalent to finding just such a line for a training set, with the hope that the line would work for any new point. This is based, like all classifiers, on the assumption that all points of a class should inhabit the same region of feature space, and that different classes should inhabit different regions of feature space. Even though the perceptron classifies a new point by performing mathematical operations on it, we can visualize the predictions it would make with this decision boundary.</p>
<p>The perceptron, like all models, does this by using intelligent mathematical operations. It performs a set of operations on any given vector in the feature space, and the output of those operations will correspond to the value of the assigned class. Below is a formal explanation of the process of classification.    </p>
<p>In its basic form the perceptron can be functionally visualized as below. </p>
<p><img src="assets/image18.png" width="430" height="250"></p>
<p>In this case, the vector x is an input vector of dimensionality $$n$$, $$x \subset\Re^n$$, whose elements of are the values $$x_1, \cdots, x_n$$. Note that a &#x2018;1&#x2019; is concatenated to the front of any input vector. These values are all multiplied by a <strong>corresponding weight</strong> ($$w_0$$ to $$w_n$$). w1 to wn are called correlating weights, and w0, which is multiplied against the &#x2018;1,&#x2019; is called the bias. These products are all summed, generating a &#x2018;<strong>weighted sum</strong>&#x2019; of the inputs. Note that this weighted sum is equal to the dot product of the x vector and a vector of the weights, $$w$$, such that</p>
<p>$$
\begin{eqnarray<em>}
w0 + x_1w_1 + x_2w_2 + \cdots + x_nw_n = \vec{w} \cdot \vec{x}
\end{eqnarray</em>}</p>
<p>$$</p>
<p>where $$\vec{x} = &lt; 1, x_1, x_2, \cdots, x_n &gt;, \vec{w} = &lt; w_0, w_1, \cdots, w_n &gt;$$</p>
<p>This weighted sum is then put through a <strong>step function</strong>, which is a function with an output space of {0,1} that has the form:</p>
<p>$$
s(x) = \left{
        \begin{array}{ll}
            1 &amp; \quad x \geq 0 \
            0 &amp; \quad x &lt; 0
        \end{array}
    \right.</p>
<p>$$</p>
<p>Put simply, this function just checks the sign of its input, and returns 0 if the input is negative, 1 otherwise.</p>
<p>Taken all together, the perceptron classification operation, or model, is simply</p>
<p>$$\hat{y} = s(\vec{w} \cdot \vec{x}) $$</p>
<p>Where the left side of the equation is the perceptron models predicted label for a point,  $$s$$ is the step function described above, and the dot product $$w \cdot x$$ is equal to the weighted sum, as described above. Notice how this operation corresponds perfectly to the diagram of the perceptron structure shown above.</p>
<p>Let&#x2019;s consider the decision boundary. Our classification operation can be visualized with the decision boundary, where the decision boundary is given simply by the equation:</p>
<p>$$\vec{w} \cdot \vec{x} = 0$$</p>
<p>Which defines a hyperplane in n dimensional space, with intercept $$w_0$$.</p>
<p><img src="assets/image20.png" width="430" height="250"></p>
<p>Notice that the region where the dot product of the input and weights is positive, which is where our classifier would return a 1, will fall above the decision boundary, and the region where the product is negative, where any point would be classified as negative, will fall below the decision boundary. </p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="./" class="navigation navigation-prev " aria-label="Previous page: What is the Perceptron?">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="perceptron-learning-algorithm.html" class="navigation navigation-next " aria-label="Next page: Perceptron Learning Algorithm">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Perceptron basics and geometric interpretation","level":"1.3","depth":1,"next":{"title":"Perceptron Learning Algorithm","level":"1.4","depth":1,"path":"perceptron-learning-algorithm.md","ref":"perceptron-learning-algorithm.md","articles":[]},"previous":{"title":"What is the Perceptron?","level":"1.2","depth":1,"path":"README.md","ref":"README.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"perceptron-basics-and-geometric-interpretation.md","mtime":"2017-03-13T06:21:39.388Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-03-13T06:24:56.625Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

